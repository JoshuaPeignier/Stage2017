\documentclass[a4paper,10pt]{llncs}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}%
\usepackage{amssymb}
\usepackage{array}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{cite}
\usepackage{color}
\usepackage{makecell}
\usepackage{caption}
%\usepackage{amsthm}
%openin
\title{}
\author{}

\pagestyle{plain}
% \newtheorem{definition}{Definition}
% \newtheorem{proposition}{Proposition}
% \newtheorem{corollary}{Corollary}
% \newtheorem{lemma}{Lemma}
% \newtheorem{example}{Example}
% \newtheorem{theorem}{Theorem}
\newtheorem{notation}{Notation}

\def\E {{\mathbb E}}
\def\EE {{\mathbb E}}
\def\FF {{\mathbb F}}
\def\II {{\mathbb I}}
\def\LL {{\mathbb L}}
\def\NN {{\mathbb N}}
\def\PP {{\mathbb P}}
\def\RR {{\mathbb R}}
\def\ZZ {{\mathbb Z}}
\def\RRpos {{\mathbb R_{\geq 0}}}
\def\RRposi {{\mathbb R_{\geq 0}^{\infty}}}
\def\Max {\textnormal {Max}}
\def\Min {\textnormal {Min}}
\def\Sup {\textnormal {Sup}}
\def\Inf {\textnormal {Inf}}


\definecolor{temp}{rgb}{0.8,0.4,0}
\definecolor{purple}{rgb}{0.8, 0, 0.8}
\newcommand\todo[1]{{\color{red}\textbf{[TO DO:  #1]}}}
\newcommand\idea[1]{{\color{purple}\textbf{[IDEA:  #1]}}}
\newcommand\notsure[1]{{\color{temp} #1}}


\begin{document}


%\maketitle
\begin{titlepage}
 \begin{center}
  % Chercher des logos
  \includegraphics[scale=0.17]{ENS.jpg}
  \includegraphics[scale=0.17]{Rennes.png}
  \includegraphics[scale=0.4]{rwth.jpg}\\[2cm]
  %\includegraphics[scale=0.5]{rwth.jpg}\\[2cm]

  \textsc{\LARGE ENS Rennes - University of Rennes 1}\\[1cm]
  \textsc{\Large Internship report: Master Degree, first year}\\[3.5cm]
  %\textsc{\Large Parcours Recherche et Innovation}\\[3.5cm]
  
  
  \textnormal{\LARGE Possibility Distribution Semantics for Probabilistic Programs with Nondeterminism}\\[2cm]
  
  \textnormal{Intern: Joshua Peignier}\\
  \textnormal{Supervisors: Benjamin Kaminski, Christoph Matheja}\\
  \textnormal{Team: Software Modeling and Verification}\\
  \textnormal{Institute: RWTH Aachen University (Aachen, Germany)}\\
  \textnormal{Dates: 15/05/2017 - 11/08/2017}\\[3cm]
 \end{center}
 

\end{titlepage}

 \begin{abstract}
In contrast to ordinary programs, probabilistic programs compute a probability distribution of output states for each given input state. One benefit of adding randomness into programs is that computationally hard problems, such as matrix multiplication or leader election protocols, can be solved (on average) more efficiently. A frequently used design concept to model unknown or overly complex probability distributions is nondeterministic choice. However, having probabilistic and nondeterministic choice within the same program (or even within loops) leads to subtle semantical intricacies. The goal of this report is to capture the semantics of both concepts uniformly using possibility theory. This would allow to simplify existing weakest-precondition style calculi for reasoning about probabilistic and nondeterministic programs.
  \begin{description}
  \item[Keywords:]Fuzzy imperative languages ; Fuzzy relations ; Possibility theory ; Weakest-precondition
  \end{description}
\end{abstract}

\section{Introduction}
\label{sec:intro}
Probabilistic programs are programs, in which probabilistic choices are involved. For instance, the simple program $\{ x := 2 \} [\frac{1}{3}] \{ x := 5 \}$ simulates flipping a biased coin (with one outcome having a probability of $\frac{1}{3}$, and the other one $\frac{2}{3}$). One easily notices that, contrary to classical deterministic programs, executing such a program contaning probabilistic choices with the same input will not always result in the same output. In this whole report, programs containing probabilistic choices will be called \emph{probabilistic programs}. Though they seem more complex than deterministic programs, probabilistic programs are useful in many ways. For example, it is simpler to solve some problems with probabilistic programs than with deterministic ones, as we can build probabilistic programs which have a better average complexity than deterministic programs (sometimes at the cost of a small probability of error). Classical examples of such programs are the quicksort algorithm \cite{Hoare61}, Freivalds' matrix multiplication \cite{Freivalds77}, and certain primality tests, such as the Miller-Rabin test \cite{Rabin77} and the Solovay-Strassen test \cite{SolovayStrassen77} (for positive numbers), and the Berlekamp test (for polynomials) \cite{Berlekamp67}. There also exists probabilistic algorithms to solve the leader election problem (see \cite{Ramanathan04}).\newline 
Apart from probabilistic choice, there exists a different kind of choice, frequently used, but also making programs nondeterministic, called nondeterministic choice. One example is the following program: $\{ x := 2 \} \square \{ x := 5 \}$. In this example, the program will set $x$ nondeterministically to $2$, or to $5$, but we cannot assign a probability to any of these choices. More precisely, it makes no sense to assign them a probability. In this whole report, programs containing such choices will be called \emph{nondeterministic programs}. Such programs share with probabilistic programs their ability to return different outputs for one given input. The nondeterministic choice was first introduced by Dijkstra in the GCL language \cite{Dijkstra76} (which did not include the probabilistic choice), and is used, for instance, in the model-checker Spin, to model the unpredictable behavior of a program where several choices are possible. It is also used in programs where the outcomes of a choice are known, but the mecanism making the choices is either unknown, or uses overly complex probability distributions. \todo{Find a source} This kind of choice is also a common concept in concurrency theory.\bigskip

However, one major problem arises when combining the probabilistic and nondeterministic choices in the same program. Programs containing both types are called \emph{fully probabilistic programs}, whereas programs containing none of them are \emph{deterministic programs}. In this report, we will use the following fully probabilistic program as a running example:
\begin{align*}
 P_0\textnormal{: } \{ \{x := 2\} \square \{x := 5\} \} [p] \{ x := 7 \} \textnormal{(with } p \in ]0,1[\textnormal{)}
\end{align*}

This program intuitively sets the program variable $x$ to $7$ with a probability $1-p$, and with a probability $p$, sets nondeterministically $x$ either to $2$ or to $5$. How can we semantically describe this program ?\bigskip



A frequently used semantics, which turned out to be well-suited to program verification, is Dijkstra's predicate transformer semantics \cite{Dijkstra76}, which consists in viewing nondeterministic programs (without probabilistic choice) as predicate transformers. More precisely, if $P$ is a program and $\varphi$ a predicate over the set of states (i.e. an object modeling a property, that each state will or not satisfy), then the predicate transformer semantics describes $P$ as an object $wp[P]$, transforming the predicate $\varphi$ (called \emph{postcondition} in this context) into another predicate called the \emph{weakest precondition of $\varphi$ for the program $P$}, denoted by $wp[P](\varphi)$. As the name explicitely says, this new predicate is the weakest (in the sense of "least restrictive", or "satisfied by the most states") predicate $\psi$ verifying the following property: if $P$ is executed in a state satisfying $\psi$, then $P$ is guaranteed to terminate in a state satisfying $\varphi$.\newline

%When introducing the GCL language \cite{Dijkstra76} (whose semantics allow it to generate nondeterministic programs, but not probabilistic programs), Dijkstra also introduced \emph{predicate transformer semantics}, and particularly the calculus of weakest precondition. This concept relies on Hoare triples: A \emph{Hoare triple} is a triple of the form $\{\psi\}P\{\varphi\}$, where $P$ is a program, and where $\psi$ and $\varphi$ are predicates respectively called \emph{precondition} and \emph{postcondition}. A Hoare triple is said to be \emph{valid} if and only if, each time that an initial state satisfies $\psi$, the final state obtained after executing $P$ satisfies $\varphi$. Thus, Dijkstra introduced the weakest preconditions: if $P$ is a program and $\varphi$ is a postcondition, then $wp[P](\varphi)$ is the weakest (in the sense of "satisfied by the most states") precondition $\psi$ making the Hoare triple $\{\psi\}P\{\varphi\}$ valid. The predicate transformer computation is a set of rules which, knowing a program $P$ and a postcondition $\psi$, allow the computation of the associated weakest precondition $wp[P](\psi)$. One can therefore see $P$ as a predicate transformer, transforming the predicate $\varphi$ into the predicate $wp[P](\varphi)$.\newline



For instance, consider the following sequential program:
\begin{align*}
 P_1\textnormal{: } x := -y ; x := x+1
\end{align*}
If we denote by $[x \geq 5]$ the predicate satisfied by all states where $x$ is greater or equal to $5$, then we get that $wp[P_1]([x \geq 5]) = wp[x := -y]([x \geq 4]) = [y \leq -4]$. (The formal method to get this result is presented in \cite{Dijkstra76} and will be reminded in Section \ref{sec:state})\bigskip

Dijkstra's predicate transformer semantics was extended by McIver and Morgan \cite{McIver05} to fully probabilistic programs by changing the semantics, such that the weakest precondition $wp[P](\varphi)$ assigns to each state $\sigma$ the probability that the execution of the program $P$ in $\sigma$ terminates in a state satisfying $\varphi$, instead of assigning only $0$ or $1$ values meaning that the execution of $P$ will certainly or certainly not terminate in a state satisfying the postcondition. Note that applying the semantics of McIver and Morgan to nondeterministic programs, without any probabilistic choice, the results are the same as in Dijkstra's semantics.\newline
By applying the semantics of McIver and Morgan to the program $P_0$ (note that, the way the program is written, the final state should not depend on the initial state in this example), we get that the probability, for each initial state, that the the final state satisfies $[x = 7]$ is $1-p$, which is intuitively expected (formally, this means that $wp[P_0]([x = 7])$ assigns to each state the constant $1-p$). Now, consider the subprogram $P_2$ of $P_0$:
\begin{align*}
 P_2\textnormal{: } \{x := 2\} \square \{x := 5\} 
\end{align*}
%The semantics of McIver and Morgan also give the following result: the probability, for each initial state, that the final state (after executing $P_0$) satisfies $[x = 2]$ is $0$ (and symmetrically for $[x = 5]$); however, the probability, for each state, that the final state (after executing $P_0$) satisfies $[x = 2 \textnormal{ OR } x = 5]$ is $p$. Theses results are not unexpected, as the execution of $P_0$ has a probability $p$ of executing $P_2$, in which we know that $x$ is set  either to $2$ or to $5$.\newline 
Recall that, the semantics of McIver and Morgan, $wp[P](\varphi)$ assigns to each state the probability that the execution of $P$ in this state terminates in a state satisfying $\varphi$ (it is therefore a function from the set of states to $[0,1]$).
As it is not possible to assign probabilities to the nondeterministic choices, conventions must be defined in order to compute weakest precondition when the program contains nondeterministic choices:
\begin{enumerate}
\item One possible choice is to consider that the outcome which is realized is the least desired outcome (for the given postcondition). In this case, the nondeterministic choice is called \emph{demonic choice} \cite{McIver05}. In this example, when computing $wp[P_2]([x = 2])$, the least desired outcome is $x := 5$. It is therefore considered as the chosen outcome, and after its execution, $[x = 2]$ cannot be satisfied. Hence, $wp[P_2]([x = 2])$ assigns to each state the constant $0$, and thus, $wp[P_0]([x = 2])$ also assigns to each state the constant $0$. We get the same result with $[x = 5]$. However, when computing $wp[P_2]([x = 2 \textnormal{ OR } x = 5])$, we get the constant function $1$ (thus, $wp[P_0]([x = 2 \textnormal{ OR } x = 5])$ is the constant function $p$), because in both possible outcomes of the choice, the postcondition will be satisfied. But this result is counterintuitive, as it violates the modularity law of probabilities (detailed below).
\item An alternative possibility is the \emph{angelic choice} (named in \cite{McIver05}), which is intuitively the opposite of the demonic choice. In this case, the outcome which is realized is the most desired outcome. With this choice, we get that $wp[P_0]([x = 2])$ is the constant function $p$, and symetrically for $wp[P_0]([x = 5])$. But $wp[P_0]([x = 2 \textnormal{ OR } x = 5])$ is also the constant function $p$. These results also violates the modularity law of probabilities.
\end{enumerate}
In both cases, counterintuitive results are obtained.
\notsure{Moreover, in the demonic choice case, in order to realise that there exists executions where $[x=2]$ can be achieved, we have to compute the weakest precondition of another postcondition (namely, $[x = 2 \textnormal{ OR } x = 5]$), because $wp[P_0]([x = 2])$ is the constant function $0$. The latter result is therefore missing information.} Therefore, in order to obtain more intuitive, or more understandable results, we can either try to define a new convention for the nondeterministic choice, or define new semantics not based on probabilities.\bigskip

%In \cite{McIver05}, the authors chose a convention such that, in this case, the probability of getting to a state satisfying $[x = 2]$ is $0$, as there is no guarantee that the corresponding instruction is executed and no way to assign a probability.\newline
%However, this result seems to lack information, as there is a possibility that the final state satisfies $[x = 2]$, but the corresponding probability is $0$, and in order to know that this program can achieve a state where $[x = 2]$, we have to compute the weakest precondition of another postcondition as $[x = 2]$ (in this case, $[x := 2 \textnormal{ OR } x = 5]$).\newline
%Moreover, the convention chosen in \cite{McIver05} (and in \cite{WuChen11}) is not universal, as other authors \cite{WuChen08,WuChen12} chose conventions with which we get other results (notably the probability of achieving a state where $[x = 2]$ holds becomes $p$ (and symmetrically for $[x = 5]$), and the probability of achieving a state where $[x = 2 \textnormal{ OR } x = 5]$ holds remains $p$ ). \bigskip

Some authors \cite{WuChen08,WuChen11,WuChen12} tried a different approach to weakest precondition calculus, based on possibility theory rather on probabilities, in order to define different semantics and possibly give a better comprehension of how nondeterministic programs behave.
Possibility theory is an alternative approach to probabilities to model uncertainty (detailed in \cite{Agarwal15,Shapiro09}). It is well-known that $Pr$ is a probability measure over $\Omega$ for the $\sigma$-algebra $\mathcal{A}$ if it is a mapping from $\mathcal{A}$ to $[0,1]$ satisfying the following axioms:
\begin{enumerate}
\item $Pr(\emptyset) = 0$
\item $Pr(\Omega) = 1$
\item $\forall U,V \in \mathcal{A}$, $Pr(U \cup V) = Pr(U) + Pr(V) - Pr(U \cap V)$ (modularity law)
\end{enumerate}
On the contrary, $\Pi$ is a possibility measure over $\Omega$ for the $\sigma$-algebra $\mathcal{A}$ if it is a mapping from $\mathcal{A}$ to $[0,1]$ satisfying the following axioms (see \cite{Agarwal15,Shapiro09} ):
\begin{enumerate}
\item $\Pi(\emptyset) = 0$
\item $\Pi(\Omega) = 1$
\item $\forall U,V \in \mathcal{A}$, $\Pi(U \cup V) = \Max( \Pi(U), \Pi(V))$ (max-modularity law)
\end{enumerate}
This type of measure, however, does not give as much information as probabilities. Indeed, when $Pr(U) = 1$, we know that an event of $U$ happens almost surely, and when $Pr(U) = 0$, we know that any event of $U$ almost never happen. However, when $\Pi(U) = 0$, we know that all events of $U$ are impossible (thus cannot happen), but when $\Pi(U) = 1$, it just means that at least one event of $U$ is totally possible, but we cannot tell that it will almost surely happen. As the name says, a possibility measure only indicates how possible an event is, but not how often it is realized. This behavior bears a strong resemblance to the nondeterministic choice, whose outcomes are known to be possible, but do not have any probability assigned. \bigskip

Using possibilities instead of probabilities first seems not to be an interesting choice, since possibilities normally do not bear as much information as probabilities, as we said earlier. But in fact, possibility theory has already been studied \cite{WuChen08,WuChen11,WuChen12} in order to define predicate transformer semantics for fuzzy imperative languages (defined in \cite{Bueno93,Bueno97}, they are languages with more instructions based on possibilities, and are at least as expressive as GCL), and the authors obtained interesting semantics in which each possible outcome is taken in account (even for GCL programs). However, these languages do not include the probabilistic choice.\bigskip

In this report, our goal is to give our own semantics of probabilistic nondeterministic programs, and more precisely an expected value semantics (which can be derived from predicate transformer semantics), where we consider expected values of fuzzy random variables (first defined in \cite{PuriRal86}, and then simplified in \cite{Shapiro09}). It is a way to get a possibility-based semantics for pGCL programs, applying the idea of \cite{WuChen08,WuChen11,WuChen12} of using possibility theory to the pGCL language presented in \cite{McIver05}.\bigskip

The report is organised as follows: In Section \ref{sec:related}, we quickly present the work of other authors using possibility theory to define semantics for language derived from GCL, or defining other semantics for pGCL. In Section \ref{sec:state}, we present the pGCL language introduced by McIver and Morgan in \cite{McIver05}, as well as their predicate transformer semantics (based on Dijkstra's semantics), and how it can be used to anticipate values of positive functions, and relate it to random variables. In Section \ref{sec:contribution}, we present our own semantics, based on the semantics of McIver and Morgan, but used to anticipate values of functions taking sets as values (to model the possibility of different values without more information, resulting of the nondeterministic choice), and explain how it counters the loss of information previously mentioned. The report is finally conclued in Section \ref{sec:conclusion}.
%The report is organised as follows: In Section \ref{sec:preliminaries}, we notably recall elementary notions such as Hoare triples and explain Dijkstra's predicate transformer semantics for GCL programs (i.e. nondeterministic programs), based on \cite{Dijkstra76}. In Section \ref{sec:state}, we present the existing work of previous authors, notably the generalisation of Dijkstra's predicate transformer semantics to pGCL programs (i.e. probabilistic nondeterministic programs) \cite{McIver05}, how the semantics can be used to compute expected values of random variables \cite{McIver05}, and the use of possibility theory in order to define other semantics for GCL programs \cite{WuChen08,WuChen11,WuChen12}. In Section \ref{sec:contribution}, we present the notion of fuzzy random variables \cite{PuriRal86,Shapiro09}, as well as our own semantics, which allows the computation of expected values of fuzzy random variables with a weakest-precondition-style calculus. The report is concluded in \ref{sec:conclusion}.

\todo{Make the notations uniform in the whole report: use $S$ (or $C$) in the rules, in the definitions, etc... And use $P$ when refering explicitely to a program defined as an example}
\section{Related Work}
\label{sec:related}

In \cite{WuChen08}, the authors consider a fuzzy imperative language \cite{Bueno93,Bueno97}, which is an extended version of GCL with more instructions based on possibility theory (notably fuzzy assignments), but which does not include the probabilistic choice. They build mathematical tools (based on domain theory, more precisely on powerdomains), and present the semantics they intend to define with the tools.
In \cite{WuChen12}, the same authors explicitely build two semantics based on the previous tools, namely a fuzzy predicate transformer semantics and a state transformer semantics, for program in the concerned fuzzy language, and prove the equivalence between the two semantic. In \cite{Tix09}, the authors develop other mathematical tools based an domain theory, and present as further work the idea of using their tools to develop a denotational semantics (otherwise known as state transformer semantics, an alternative semantics type to the predicate transformer semantics; both types are dual) for programs including probabilities and nondeterminism. (It is, to our knownledge, the only other attempt to define clearer semantics for languages including probabilities AND nondeterminism, but it focuses on another style of semantics.)



%\section{Preliminaries}
%\label{sec:preliminaries}
%In this section, we consider only nondeterministic programs in the GCL language. We present the GCL language, define more formally the Hoare triples and their validity, and detail Dijkstra's weakest-precondition computation rules \cite{Dijkstra76} on which the existing semantics (presented in Section \ref{sec:state}), as well as our own semantics (presented in Section \ref{sec:contribution}) are based.
    
%    \subsection{The GCL Language}
%    Let $Vars$ be the set of variables of one program, and $Vals$ the set of values to which the variables can be assigned. The state space $\Sigma$ is the set of mappings from $Vars$ to $Vals$: $$\Sigma = \{\sigma \,|\, \sigma : Vars \rightarrow Vals \}$$
%     This is a way to model the fact that each state is characterized uniquely by the value of each variable in the program.\bigskip
%
%The GCL language, presented in \cite{Dijkstra76}, is defined by the following grammar\footnote{The original GCL included an \texttt{abort} instruction corresponding to an error case. We chose, not to include it, as it can be represented by $\texttt{while} (\texttt{true})\{ \texttt{skip} \}$}\footnote{Dijkstra did not define GCL with this exact grammar, but introduced nondeterminism in guarded commands. But the grammar presented here, adapted from \cite{McIver05}, is equivalent.}:
%
%
%\begin{align*}
% S ::= & \texttt{ skip } \,|\, x := A \,|\, S;S  \,|\, \{S\} \square \{S\} \,|\ \\
% & \texttt{ if } (b) \texttt{ then } \{ S \} \texttt{ else } \{ S \} \,|\, \texttt{ while }(b) \texttt{ do }\{S\} \\
%\end{align*}
%
%This language is sufficient to describe any nondeterministic program. The \texttt{if\dots then\dots else} and \texttt{while} structure are the usual control structures encountered in many languages. The \texttt{skip} instruction corresponds to an empty instruction, where nothing is done (it is included in the language so that we do not have to create a specific \texttt{if\dots then\dots} instruction for the case where no \texttt{else} is needed.) $x := A$ is an assignment statement, where the program variable $x$ (element of $Vals$) is set to the value of the expression $A$ in the current state. $S_1 ; S_2$ is a sequence assignment, modeling the execution of $S_1$ followed by $S_2$. The $\{S_1\} \square \{S_2\} $ is a statement that will nondeterministically execute either $S_1$ or $S_2$. Recall that no probability can be assigned to $S_1$ or $S_2$ because it makes no sense. This means that if $\{S_1\} \square \{S_2\} $ is executed a large number of times, no clear pattern should appear.\newline
%Note that this grammar only has a choice instruction with two choices; but combination of instructions make possible the realisation of choices with any finite number (and even a countable number when using the loop) of outcomes.\bigskip
     
%	\subsection{Hoare triples}
%     
%     When model-checking programs, we need a way to define properties that are satisfied or not, depending on the state of the program. We can use the following definition of predicates.
%     \begin{definition} 
%     \begin{itemize}
%     \item A \emph{predicate} is a mapping $\varphi : \Sigma \rightarrow \{0,1\}$.
%     A state $\sigma$ is said to \emph{satisfy the predicate} $\varphi$ if $\varphi(\sigma) = 1$. (Note that $\varphi$ may also be viewed simply as a subset of $\Sigma$. )
%     \item A partial order can be define over predicates: $\varphi_1 \leq \varphi_2$ if $\forall \sigma \in \Sigma$, $\varphi_1(\sigma) \leq \varphi_2(\sigma)$. (This partial order is equivalent to the inclusion, when considering predicates as subsets of $\Sigma$.)
%     \end{itemize}
%     \end{definition}
%      
%      We also need a formal way to define the specification of a program, which is usually considered as a Hoare triple:
%      \begin{definition}
%      A \emph{Hoare triple} is a triple $\{\psi\}P\{\varphi\}$ where $P$ is a program, and $\psi$ and $\varphi$ are two predicates respectively called pre- and postcondition.
%      \end{definition}
%      We expect the specification of a program (when it is correct) to indicate that, if the precondition holds and the program is executed, then the postcondition holds after the execution. This is formalised by the validity of Hoare triples:
%      \begin{definition}
%      A Hoare triple $\{\psi\}P\{\varphi\}$ is said to be \emph{valid} if each time that $P$ is executed in a state satisfying $\psi$, then $P$ terminates in a state satisfying $\varphi$.
%      (Note that because of nondeterminism, the execution of $P$ in one state may result in several possible states. Therefore, we expect that, if $\sigma$ satisfies $\psi$, then all states in which $P$ can result when executed in $\sigma$ satisfy $\varphi$.)
%      \end{definition}
     
     
	
%	\subsection{Predicate transformer semantics and weakest precondition calculus}
%	Dijkstra showed that there exists, for a given program $P$ and a given postcondition $\varphi$, a particular precondition $wp[P](\varphi)$ called the \emph{weakest precondition}, greater (for the previously defined partial order) than all preconditions $\psi$ making the Hoare triple $\{\psi\}P\{\varphi\}$ valid. It is named this way, since it is indeed the weakest (in the sense of "less restrictive", or "satisfied by the most states") precondition making the Hoare triple valid.\bigskip
%
%It is then possible to see $wp[P]$ as a predicate transformer, transforming a postcondition $\varphi$ into the weakest-precondition $wp[P](\varphi)$. The rules of transformation (adapted from \cite{McIver05}) are given in Table \ref{table:rules_wp_gcl}. Recall that predicates are mappings from $\Sigma$ to $\{0,1\}$. The state $\sigma[x/A]$ is the state obtained when setting the variable $x$ to the value of $A$ in $\sigma$; $[\![ b : true ]\!] $ is a particular predicate, evaluating to $1$ in $\sigma$ if and only if the boolean variable $b$ is true in $\sigma$ (and symmetrically for $[\![ b : false ]\!] $). Finally, the form lfp ($\lambda X. f(X)$) denotes the least fixed point of the application $f$ (its value can be computed with Kleene's fixed point theorem). \todo{Do not forget to precise that in order for fixed points to exists, it suffices to have a complete partial order, and that it is the case here. Find a reference for Kleene's theorem.}\bigskip
%\begin{table}
%\begin{center}
%\begin{tabular}{|p{3cm}|p{9cm}|}
% \hline
% \thead{$P$} & \thead{$wp[P](\varphi)$} \\
% \hline
% \thead{\texttt{skip}} & \thead{$\varphi$} \\
% \hline
% \thead{$x := A$} & \thead{$\lambda\sigma.\varphi(\sigma[x/A])$} \\
% \hline
% \thead{$P_1 ; P_2$} & \thead{$wp[P_1](wp[P_2](\varphi))$} \\
% \hline
% \thead{$\{P_1\} \square \{P_2\}$} & \thead{$\Min(wp[P_1](\varphi),wp[P_2](\varphi))$} \\
% \hline
% \thead{$\texttt{if } (b) \texttt{ then } \{ P_1 \}$ \\ $\texttt{ else } \{ P_2 \}$} & \thead{$\lambda\sigma. [\![b : true ]\!](\sigma) \cdot wp[P_1](\varphi)(\sigma) + [\![b : false ]\!](\sigma) \cdot wp[P_2](\varphi)(\sigma)$} \\
% \hline
% \thead{$\texttt{while }(b) \texttt{ do }\{P\}$} & \thead{lfp ($\lambda X. (\lambda\sigma. [\![b : true ]\!](\sigma) \cdot wp[P_1](X)(\sigma) + [\![b : false ]\!](\sigma) \cdot \varphi(\sigma))$)} \\
% \hline
%\end{tabular}
%\end{center}
%\caption{Rules for defining the predicate transformer $wp$}
%\label{table:rules_wp_gcl}
%\end{table}
%
%Applying these rules to the example program $P_1$ from Section \ref{sec:intro} with the postcondition $[x \geq 5]$ (evaluating to $1$ in state $\sigma$ if and only if $x \geq 5$ holds in $\sigma)$, we get:
%\begin{align*}
%& \textnormal{ }wp[x := - y ; x := x+1]([x \geq 5]) = wp[x := - y ](wp[x := x+1]([x \geq 5])) \\
% =& \textnormal{ }wp[x := - y ]([x \geq 4]) = [-y \geq 4] = [y \leq -4] \\
%\end{align*}
	
\section{State of the art: the language, and McIver and Morgan's semantics}
\label{sec:state}
In this section, we present the pGCL language and the vision of McIver and Morgan over weakest precondition calculus \cite{McIver05}, and explain (\todo{insert reference here}) how this type of calculus can be exploited to compute expected values of random variables.
	\subsection{The pGCL language}
	The pGCL language is an extension of Dijkstra's GCL. It was first introduced in \cite{McIver05}, and includes a new instruction that GCL did not include: the probabilistic choice. Therefore, the pGCL language allows the generation of fully probabilistic programs. It is defined by the following grammar\footnote{The original GCL included an \texttt{abort} instruction corresponding to an error case. We chose, not to include it, as it can be represented by $\texttt{while } (\texttt{true}) \texttt{ do }\{ \texttt{skip} \}$}\footnote{The rules for nondeterminism were defined by Dijkstra in guarded commands \cite{Dijkstra76}, but McIver and Morgan used an equivalent grammar and added probabilistic choice to get the grammar presented here. }:\bigskip


\begin{align*}
 S ::= & \texttt{ skip } \,|\, x := A \,|\, S;S  \,|\, \{S\} \square \{S\} \,|\ \{S\} [p] \{S\} \,|\ \\
 & \texttt{ if } (b) \texttt{ then } \{ S \} \texttt{ else } \{ S \} \,|\, \texttt{ while }(b) \texttt{ do }\{S\} \\
\end{align*}

This language is sufficient to describe any fully probabilistic program. The \texttt{if\dots then\dots else} and \texttt{while} structure are the usual control structures encountered in many languages. The \texttt{skip} instruction corresponds to an empty instruction, where nothing is done (it is included in the language so that we do not have to create a specific \texttt{if\dots then\dots} instruction for the case where no \texttt{else} is needed.) $x := A$ is an assignment statement, where the program variable $x$ is set to the value of the expression $A$ in the current state. $S_1 ; S_2$ is a sequence assignment, modeling the execution of $S_1$ followed by $S_2$. We take interest primarly in the two remaining instructions.\newline
The $\{S_1\} [p] \{S_2\}$ (where $p \in ]0,1[$) instruction models the fact that there is a probability $p$ that $S_1$ is executed, and a probability $1-p$ that $S_2$ is executed. Finally, $\{S_1\} \square \{S_2\} $ is a statement that will nondeterministically execute either $S_1$ or $S_2$. The difference with the probabilistic choice is that no probability can be assigned to $S_1$ or $S_2$ because it makes no sense. This means that if $\{S_1\} \square \{S_2\} $ is executed a large number of times, no clear pattern should appear, whereas the execution of $S_1 [p] S_2$ a large number of times will tend to have a proportion $p$ of cases where $S_1$ is executed, and a proportion $1-p$ of cases where $S_2$ is executed.\newline
Note that this grammar only has choice instructions with two outcomes; but combination of instructions make possible the realisation of choices with any finite number (and even a countable number when using loops) of outcomes.\bigskip

	\subsection{Predicate transformer semantics and weakest precondition calculus}
	Let $\Sigma$ be the set of program states. Each state is uniquely characterized by the values of each variable in the state.\bigskip
	
	\begin{definition}{\textnormal{(Predicate and binary predicate)}}
	\item A \emph{predicate} $\varphi$ is a function from $\Sigma$ to $[0,1]$. One says that each state $\sigma$ has a probability $\varphi(\sigma)$ of satisfying $\varphi$.
	\item When $\varphi$ takes only the values $0$ and $1$, it is called a \emph{binary predicate}. In this case, one abusively says that $\sigma$ satisfy $\varphi$ if and only if $\varphi(\sigma) = 1$.
	\end{definition}

    Let $P$ be a pGCL program and let $\varphi$ be a binary predicate.
	What is the probability that the execution of $P$ in a state $\sigma$ is guaranteed to terminate in a state satisfying the postcondition $\varphi$ ? McIver and Morgan extended Dijkstra's vision of weakest precondition calculus, such that $wp[P](\varphi)(\sigma)$ is the answer, with the following definition:
	\begin{definition}
	If $P$ is a pGCL program, then the predicate transformer semantics of $P$ is given by $wp[P]$ (defined by the rules\footnote{These rules correspond to the case where the nondeterministic choice is considered to be the demonic choice, as Dijkstra first intended.} in Table \ref{table:rules_wp_pgcl}).\newline
	$wp[P]$ is a predicate transformer, i.e. for all postconditions $\varphi$, $wp[P](\varphi)$ is a predicate, such that $\forall \sigma \in \Sigma$, $wp[P](\varphi)(\sigma)$ is the probability that the execution of $P$ in $\sigma$ terminates in a state satisfying $\varphi$
	\end{definition}
	Therefore, $P$ is described by the semantics of McIver and Morgan as an object $wp[P]$ transforming the predicate $\varphi$ into the predicate $wp[P](\varphi)$.
	
	\begin{table}
\begin{center}
\begin{tabular}{|p{3cm}|p{9cm}|}
 \hline
 \thead{$P$} & \thead{$wp[P](\varphi)$} \\
 \hline
 \thead{\texttt{skip}} & \thead{$\varphi$} \\
 \hline
 \thead{$x := A$} & \thead{$\lambda\sigma.\varphi(\sigma[x/A])$} \\
 \hline
 \thead{$P_1 ; P_2$} & \thead{$wp[P_1](wp[P_2](\varphi))$} \\
 \hline
 \thead{$\{P_1\} \square \{P_2\}$} & \thead{$\lambda\sigma. \Min(wp[P_1](\varphi)(\sigma),wp[P_2](\varphi)(\sigma))$} \\
 \hline
 \thead{$\{P_1\} [p] \{P_2\}$} & \thead{$\lambda\sigma. p\cdot wp[P_1](\varphi)(\sigma) + (1-p)\cdot wp[P_2](\varphi)(\sigma)$} \\
 \hline
 \thead{$\texttt{if } (b) \texttt{ then } \{ P_1 \}$ \\ $\texttt{ else } \{ P_2 \}$} & \thead{$\lambda\sigma. [\![b : true ]\!](\sigma) \cdot wp[P_1](\varphi)(\sigma) + [\![b : false ]\!](\sigma) \cdot wp[P_2](\varphi)(\sigma)$} \\
 \hline
 \thead{$\texttt{while }(b) \texttt{ do }\{P\}$} & \thead{lfp ($\lambda X. (\lambda\sigma. [\![b : true ]\!](\sigma) \cdot wp[P_1](X)(\sigma) + [\![b : false ]\!](\sigma) \cdot \varphi(\sigma))$)} \\
 \hline
\end{tabular}
\end{center}
\caption{Rules for defining the predicate transformer $wp$}
\label{table:rules_wp_pgcl}
\end{table}
The state $\sigma[x/A]$ is the state obtained when syntactically replacing the variable $x$ by the value of $A$ in $\sigma$; $[\![ b : true ]\!] $ is a binary predicate, evaluating to $1$ in $\sigma$ if and only if the boolean expression $b$ is true in $\sigma$ (and symmetrically for $[\![ b : false ]\!] $). Finally, the form lfp ($\lambda X. f(X)$) denotes the least fixed-point of the application $f$ (where the $X$ are predicates). The value of the least fixed-point can be computed with Kleene's fixed-point theorem (under certain hypotheses, verified here; more details in Appendices \ref{subsec:kleene} and \ref{subsec:predicates_cpo}).\bigskip

\begin{remark} We will often simplify the expressions in the table by writing for instance $\Min(wp[P_1](\varphi),wp[P_2](\varphi))$ instead of $\lambda\sigma. \Min(wp[P_1](\varphi)(\sigma),wp[P_2](\varphi)(\sigma))$, or $p\cdot wp[P_1](\varphi) + (1-p)\cdot wp[P_2](\varphi)$ instead of $\lambda\sigma. p\cdot wp[P_1](\varphi)(\sigma) + (1-p)\cdot wp[P_2](\varphi)(\sigma)$. But recall that these objects are predicates.
\end{remark}
	
	\notsure{Note that the rules allow the computation of $wp[P](\varphi)$ for all arbitrary predicates $\varphi$, but in the practice, we only want to compute $wp[P](\varphi)$ where $\varphi$ is a binary predicate, because when checking programs, we want to verify whether the final states satisfy a property or not, and not to verify whether a final states satisfies a property with a certain probability. In the only cases where $\varphi$ is not a binary predicate, $\varphi$ will result from the computation of another weakest precondition $wp[P'](\psi)$} \bigskip
	



Applying these rules to the example program $P_1$ from Section \ref{sec:intro} with the postcondition $[x \geq 5]$ (binary predicate evaluating to $1$ in state $\sigma$ if and only if $x \geq 5$ holds in $\sigma)$, we get (the details with $\lambda$-calculus are left to the reader):
\begin{align*}
 \textnormal{ }wp[x := - y ; x := x+1]([x \geq 5]) = wp[x := - y ](wp[x := x+1]([x \geq 5]))
\end{align*}
The assignment rules determines that $wp[x := x+1]([x \geq 5]) = \lambda\sigma. [x \geq 5](\sigma[x/x+1])$.
It turns out that:
\begin{align*}
wp[x := x+1]([x \geq 5]) = [x \geq 4]
\end{align*}
\begin{proof}
Recall that $[x \geq 5](\sigma) = 1$ if $x \geq 5$ in $\sigma$ and $0$ else. Therefore $[x \geq 5](\sigma[x/x+1]) = 1$ if $x \geq 5$ in $\sigma[x/x+1]$ and $0$ else. This is equivalent to: $[x \geq 5](\sigma) = 1$ if $x+1 \geq 5$ in $\sigma$ and $0$ else. Therefore, $[x \geq 5](\sigma[x/x+1]) = [x+1 \geq 5](\sigma)$, and $wp[x := x+1]([x \geq 5]) = [x+1 \geq 5] = [x \geq 4]$.
\end{proof}
 We resume the computation:
\begin{align*}
& \textnormal{ }wp[x := - y ; x := x+1]([x \geq 5]) = wp[x := - y ](wp[x := x+1]([x \geq 5])) \\
 =& \textnormal{ }wp[x := - y ]([x+1 \geq 5]) = \textnormal{ }wp[x := - y ]([x \geq 4]) = [-y \geq 4] = [y \leq -4] \\
\end{align*}

	One can also apply these rules to the program $P_0$ and get the results presented in Section \ref{sec:intro}. The calculation is done in Appendix \ref{subsec:wp_p0}. \todo{Maybe move this calculation here if there is enough place. It should be more logical, since it is our running example.}
	
	\subsection{Anticipating values, link with random variables}
    \label{subsec:anticipation}
    
	In the practice, weakest precondition calculus can be used to anticipate the value of certain functions. Recall that with the simple program $P_1$ given in Section \ref{sec:intro}, we proved that $wp[P_1]([x \geq 5]) = [y \leq -4]$. As $P_1$ is deterministic, we get that, if $\sigma$ satisfies $[y \leq -4]$, then the state obtained after executing $P_1$ in $\sigma$ satisfies $[x \geq 5]$; and if $\sigma$ does not satisfy $[y \leq -4]$, then the state obtained after executing $P$ in $\sigma$ does not satisfy $[x \geq 5]$. We can say that the function $[y \leq -4]$ anticipates the value of $[x \geq 5]$ after the execution of $P_1$.\bigskip
	
	\todo{Write references for what follows}
It is in fact possible, in the deterministic case, to anticipate values of other functions, such as the function associating to $\sigma$ the value of $x^2$ in $\sigma$, or the value of $|\textnormal{sin}(y)|$ in $\sigma$. However, due to certain constraints in order to ensure the existence of least fixed-points and make the computation possible, we must restrict ourselves to functions from $\Sigma$ to $\RRposi$ (where $\RRposi = [0;+\infty]$). Let $\mathcal{F} = \{f \,|\, f : \Sigma \rightarrow \RRposi \}$. \footnote{With this restriction, a complete partial order can be defined on $\mathcal{F}$ and Kleene's fixed point theorem can be applied. See Appendices \ref{subsec:kleene} and \ref{subsec:predicates_cpo}} That restriction is not a problem in practice, as one can always decompose an arbitrary function as two functions, namely, its absolute value and its sign (and carefully redefine all operations between functions, such as $+$, $\times$, etc.). \bigskip

Therefore, it is possible to extend the previous predicate transformer semantics in an "anticipated-value" semantics for deterministic programs, by viewing programs as function transformers $\mathcal{F} \rightarrow \mathcal{F}$.
The anticipation of values can be extended to nondeterministic programs (by considering that, in the nondeterministic choice, the anticipated value is the least one), and to fully probabilistic programs, by taking the ponderated mean between the two possible values. The rules of computation are rigorously the same as in Table \ref{table:rules_wp_pgcl}. But in this case, we apply them to a function $f \in \mathcal{F}$, and obtain another function $wp[P](f) \in \mathcal{F}$. (Note that predicates are also elements of $\mathcal{F}$. Hence, the previous computations still work in this context.)

\begin{notation} In the following, we will denote by $x_\sigma$ the value of the program variable $x$ in $\sigma$, and we will assimilate the program variable $x$ to the function $\underline{x} : \sigma \rightarrow x_\sigma$. (This is a simplified notation. In the practice, we should consider one function giving the absolute value of $x_\sigma$ and another giving its sign.) Hence, $\underline{x} \in \mathcal{F}$, it is a function giving the value of $x$ in each state.
\end{notation}

\begin{example} Consider the program $P_1$ from Section \ref{sec:intro}, and the function $\underline{x}^2 \in \mathcal{F}$. Its value after the execution of $P_1$ can be computed with:
\begin{align*}
& \textnormal{ }wp[x := - y ; x := x+1](\underline{x}^2) = wp[x := - y ](wp[x := x+1](\underline{x}^2)) \\
 =& \textnormal{ }wp[x := - y ]((\underline{x}+1)^2) = (1-\underline{y})^2  \\
\end{align*}
For example, if $(1-\underline{y})^2(\sigma) = 4$, then $\sigma$ is a state where $(1-y_\sigma)^2 = 4$, for instance where $y_\sigma = 3$. Then by executing $P_1$ in $\sigma$, $x$ is set to $-3$, and then to $-2$. In the final state $\sigma'$, we have $x_{\sigma'}^2 = 2$. Therefore, $(1-\underline{y})^2$ anticipates the value that $\underline{x}^2$ takes after the execution of the program.
\end{example}

\begin{example} Consider the program $P_0$ from Section \ref{sec:intro}, we can anticipate the value of the variable $x$ by computing the anticipate value of the function $\underline{x}$ (we should normally anticipate the value of $\underline{|x|}$, but we easily see that they are the same in this context):
\begin{align*}
wp[P_0](\underline{x}) &= wp[ \{ \{x := 2\} \square \{x := 5\} \} [p] \{ x := 7 \} ](\underline{x}) \\
&= p\cdot wp[ \{x := 2\} \square \{x := 5\} ](x) + (1-p)\cdot wp[x := 7](\underline{x}) \\
&= p\cdot\Min(wp[x := 2](\underline{x}),wp[x := 5](\underline{x})) + (1-p)\cdot wp[x := 7](\underline{x}) \\
&= \lambda\sigma. p\cdot\Min(wp[x := 2](\underline{x})(\sigma),wp[x := 5](\underline{x})(\sigma))\\ & + (1-p)\cdot wp[x := 7](\underline{x})(\sigma) \\
&= \lambda\sigma. p\cdot\Min(\underline{x}(\sigma[x/2]),\underline{x}(\sigma[x/5])) + (1-p)\cdot \underline{x}(\sigma[x/7]) \\
&= \lambda\sigma. p\cdot\Min(2,5) + (1-p)\cdot 7 \\
&= \lambda\sigma. 2p+7(1-p)
\end{align*}
\end{example}
We leave the result in this form to underline interesting remarks.

\begin{remark}
For each $\sigma$, $wp[P_0](\underline{x})(\sigma)$, which is the anticipated value of $x$ after the execution of $P_0$ in $\sigma$, has the form of the expected value $E(X)$ of a positive random variable $X$ taking the value $2$ with the probability $p$, and the value $7$ with the probability $1-p$.\newline
\end{remark}
\begin{theorem}
Let $P$ be a probabilistic program (without nondeterminism), let $\sigma$ be an initial state, and let $f \in \mathcal{F}$ be a function. For each $\omega$, the execution of $P$ in $\omega$ can be informally divided as several "branches", called execution schemes $\omega$ (at each choice, the current branch is divided into two branches). We consider an execution scheme to be a full execution, i.e. at the end of an $\omega$, $P$ terminated, or is looping infinitely. Let $\Omega_\sigma$ be the set of execution schemes obtained when executing $P$ in $\sigma$. A probability can be assigned to each $\omega \in \Omega_\sigma$. We can therefore see the value of $f$ after the execution of $P$ in $\sigma$ as a positive random variable $X_\sigma : \Omega_\sigma \rightarrow \RRposi$.\newline
Thus, $wp[P](f) = \lambda\sigma.E(X_\sigma)$.
\end{theorem}
This result can be more clearly observed in Example \ref{ex:diff_rv} in Subsection \ref{subsec:ex_ant}.
\todo{Maybe move the example here.}
\begin{remark}
The previous theorem can be extended to fully probabilistic programs by applying rules in Table \ref{table:rules_wp_pgcl} and by considering the nondeterministic choice as the demonic choice. Note however that the $\Omega_\sigma$ that we obtain depend on the postcondition we are computing (as shown in Example \ref{ex:diff_ex_sch} in Subsection \ref{subsec:ex_ant}), because the the demonism in nondeterministic choice will restrict each of these choices to one outcome, depending on the postcondition.\newline
\end{remark}
\begin{remark}
The result of the computation of $wp[P_0](\underline{x})$ still does not take in account the possibility that $x$ can take the value $5$.
Symmetrically, if the nondeterministic choice was to be considered as the angelic choice, we would get that $wp[P_0](\underline{x}) = 5p+7(1-p)$ and one result would be missing.
\end{remark}

These results are therefore counterintuitive. At this point, we defined our own semantics by considering fuzzy random variables, instead of random variables.
	
%	\subsection{Chinese work}
%	\todo{Obviously change this title when you have a better idea.}
%\temp{
%In this whole report, we will primarly consider the pGCL language, presented in \cite{McIver05}, and defined by the following grammar\footnote{The original pGCL included an \texttt{abort} instruction corresponding to an error case. We chose, not to include it, as it can be represented by $\texttt{while} (\texttt{true})\{ \texttt{skip} \}$}:
%
%
%\begin{align*}
% S ::= & \texttt{ skip } \,|\, x := A \,|\, S;S \,|\, \{S\} [p] \{S\} \,|\, \{S\} \square \{S\} \,|\ \\
% & \texttt{ if } (b) \texttt{ then } \{ S \} \texttt{ else } \{ S \} \,|\, \texttt{ while }(b) \texttt{ do }\{S\} \\
%\end{align*}
%
%This language is sufficient to describe any probabilistic or nondeterministic program. The \texttt{if\dots then\dots else} and \texttt{while} structure are the usual control structures encountered in many languages. The \texttt{skip} instruction corresponds to an empty instruction, where nothing is done (it is included in the language so that we do not have to create a specific \texttt{if\dots then\dots} instruction for the case where no \texttt{else} is needed.) $x := A$ is an assignment statement, where the program variable $x$ is set to the value of the expression $A$ in the current state. $S_1 ; S_2$ is a sequence assignment, modeling the execution of $S_1$ followed by $S_2$. We take interest primarly in the two remaining instructions.\newline
%The $\{S_1\} [p] \{S_2\}$ instruction models the fact that there is a probability $p$ that $S_1$ is executed, and a probability $1-p$ that $S_2$ is executed. Finally, the $\{S_1\} \square \{S_2\} $ (first introduced by Dijkstra in \cite{Dijkstra76}) is a statement that will nondeterministically execute either $S_1$ or $S_2$. The difference with the probabilistic choice is that no probability can be assigned to $S_1$ or $S_2$ because it makes no sense. This means that if $\{S_1\} \square \{S_2\} $ is executed a large number of times, no clear pattern should appear, whereas the execution of $S_1 [p] S_2$ a large number of times will tend to have a proportion $p$ of cases where $S_1$ is executed, and a proportion $1-p$ of cases where $S_2$ is executed.\newline
%Note that we are using choice instructions with only two choices, but combination of instructions make possible the realisation of choices with any finite number (and even a countable number when using the loop) of outcomes.\bigskip
%}

\section{A new semantics based on possibilities}
\label{sec:contribution}

In this section, we consider programs over the pGCL language presented in Section \ref{sec:state}. As we stated before, previous existing semantics give nonsatisfying results, both when computing weakest preconditions of predicates in fully probabilistic programs and when anticipating values of functions in fully probabilistic programs. Our goal is to define a new semantics, which should be based on possibility theory, and would more clearly represent each possible outcome.  Recall that we mentioned in Section \ref{sec:state} that weakest-precondition calculus could be used in order to anticipate values of functions of $\mathcal{F}$ (i.e. functions from $\Sigma$ to $\RRposi$), and that the corresponding computation in fact gave, for each state $\sigma$, the expected value of a random variable (i.e. the value of the function we are anticipating, after the execution of the program in $\sigma$). Our main idea here is to replace functions from $\Sigma$ to $\RRposi$ by functions from $\Sigma$ to $\mathcal{P}(\RRposi)$, and to identify the anticipation of their values as the expected value of \textit{Fuzzy random variable} (introduced in \cite{PuriRal86}, and simplified in \cite{Shapiro09}).

\subsection{Fuzzy random variables}
%Let $\Sigma$ be the set of states. We consider that the variables in our programs can only take \underline{real positive} values.\footnote{This is due to constraints explained further. But note that we can implement an arbitrary real variable by two variables, one for its absolute value, and one for its sign.} 
We adapted the following definition from \cite{PuriRal86}.

\begin{definition}{\textnormal{(Fuzzy Random Variable)\newline}}
 A Fuzzy Random Variable (short: FRV) is a mapping $X$ from a probabilistic space $(\Omega,\mathcal{A},Pr)$ to the powerset $P(\RRposi)$. This means that, if $X$ is an FRV, then for all $\omega \in \Omega$, $X(\omega)$ is a subset of $\RRposi$. In fact, it is a set of possible values for $X$ in the event $\omega$.
\end{definition}

In \cite{PuriRal86}, there exists a definition of the expected value $E(X)$ of an arbitrary fuzzy random variable $X$ over an arbitrary probabilistic space. However, in our context, it suffices to consider $\Omega$ as a countable space $\{\omega_1, \omega_2, \dots \}$ (recall that our events are execution schemes) and fuzzy random variables taking as values only sets of positives values (as we previously defined them). Then, by using the definition of expected value in \cite{PuriRal86} and applying our simplifications, we come to the following definition of the expected value, which is sufficient in our context:

\begin{definition}
If $X$ is a fuzzy random variable from $\Omega$ to $\RRposi$, then we can define the expected value $E(X)$ as follows:
$$E(X) = \{ \sum_{n = 1}^{+\infty} t_iPr(\omega_i) \,|\, \forall i \in \NN^*, t_i \in X(\omega_i) \} \subset \RRposi$$
\end{definition}
One can interprete this definition as follows: if $X$ is a positive fuzzy random variable (i.e. a random variable associating to each event a set of positive values), then $E(X)$ is the set obtained by computing all possible (even infinite) averages (by picking one value in each $X(\omega_i)$).

%\begin{example}
% In the following, for each program variable $x$, we denote by $\underline{x}$ the FRV such that, for all $\sigma \in \Sigma$, $\underline{x}(\sigma) = \{ x_{\sigma} \}$, meaning that $\underline{x}$ associates to each state $\sigma$ the singleton containing the value $x_\sigma$, which is the value of the program variable $x$ in the set $\sigma$. (Recall that each state is characterized uniquely by a tuple containing the values of each variable in a given order.). Therefore, we can say that the FRV $\underline{x}$ models the program variable $x$.
%\end{example}
%
%A fuzzy random variable $X$ over a program can model several concepts. For instance, it can be used to model the runtime of a program executed in a state $\sigma$ (which is why we chose to include $\infty$); in this example, $X(\sigma)$ would give all possible runtimes for the execution of one program starting from $\sigma$. It can also model the value of a program variable (or any function over the program variables) after the execution of a program.\newline

% For instance, for any program variable $x$, with the previous notations, for all $\sigma \in \Sigma$, $\underline{x}(\sigma) = \{ x \}$ is the expected value of the program variable $x$ after executing the program $\texttt{skip}$ in $\sigma$.\newline
% As all previous computations can be obtained from program variables (for instance, the runtime of a program (in the sense of "number of instructions executed before termination") can be obtained by adding a program variable $t$ representing the current runtime, initially $0$, and incrementing it at each instruction), we will in the following only consider computation of expected values of fuzzy random variables characterizing the value of a program variable after the execution of the program in one state.\newline

\subsection{Contribution: the weakest-precondition-style calculus of anticipation of values of set-valued functions, link with fuzzy random variables}
Let $\mathcal{F}' = \{f \,|\, f : \sigma \rightarrow P(\RRposi) \}$ be the new set of functions whose values we want to anticipate. What it intuitively means is that $f$ maps each $\sigma$ to a set of "possible values". (As we previously explained, restriction to positive values only is not a problem in the practice.)
\begin{example}
In the following, we redefine the application $\underline{x}$ by : $\underline{x} = \lambda\sigma. \{x_\sigma\}$, meaning that $\underline{x}$ associates to $\sigma$ a set containing the only possible value for $x$ in this state (which is directly accessible, since each state is characterized uniquely by the values of each variable).
\end{example}

We replace the $wp$ operator by the following operator:
\begin{definition}
 	If $P$ is a pGCL program, then our \todo{find a proper name} semantics of $P$ is given by $ev[P]$ (defined by the rules in Table \ref{table:rules_ev}).\newline
	$ep[P]$ is a function transformer, i.e. for all functions $f \in \mathcal{F}'$, then $ev[P](f) \in \mathcal{F}'$.
\end{definition}

Recall that we mentioned in Subsection \ref{subsec:anticipation} that, if $f \in \mathcal{F}$ and if $P$ is a pGCL program, then the value of $f$ after the execution of $P$ in a state $\sigma$ was given by a random variable $X_\sigma$, and $wp[P](f)$ in fact gave $E(X_\sigma)$ for each $\sigma$.\newline
Symmetrically, here, if $f \in \mathcal{F}'$, then the value of $f$ (which is a set) after the execution of $P$ in a state $\sigma$ is given by a fuzzy random variable $X_\sigma$, and $ev[P](f)$ in fact gives $E(X_\sigma)$ (which is a set) for each $\sigma$. This confirms that $ev[P]$ transforms a function of $\mathcal{F}'$ in another function of $\mathcal{F}'$.\newline
%Concretely, if $X$ is a FRV (i.e. an application mapping each state $\sigma$ to a subset $X(\sigma)$ of $\RRposi$), then $ev[S](X)(\sigma)$ is the set of possible expected values of the FRV $X$ after executing $S$ in the state $\sigma$.\newline
%For instance, if we consider a program variable $x$, then the FRV $\underline{x}$ maps each state $\sigma$ to the set $\{x_\sigma\}$. Our goal is that the FRV $\underline{x}$ is transformed by $ev[S]$ into the FRV $ev[S](\underline{x})$, which maps each $\sigma$ to the set $ev[S](\underline{x})(\sigma)$ of possible values of the variable $x$ after executing $S$ in the state $\sigma$. Therefore, we can say that $ev[S](\underline{x})(\sigma)$ is the \emph{expected value} (or rather \emph{set of possible expected values}) of the FRV $\underline{x}$ after executing $S$ in the state $\sigma$; or, with other words, the set of possible expected values of the program variable $x$ after executing $S$ in $\sigma$.\newline


\begin{table}
\begin{center}
\begin{tabular}{|p{3cm}|p{9cm}|}
 \hline
 \thead{$S$} & \thead{$ev[S](f)$} \\
 \hline
 \thead{\texttt{skip}} & \thead{$f$} \\
 \hline
 \thead{$x := A$} & \thead{$\lambda\sigma.f(\sigma[y/A])$} \\
 \hline
 \thead{$S_1 ; S_2$} & \thead{$ev[S_1](ev[S_2](f))$} \\
 \hline
 \thead{$S_1 [p] S_2$} & \thead{$\lambda\sigma.\{t_1 p+t_2(1-p) \,|\, t_1 \in ev[S_1](f)(\sigma), t_2 \in ev[S_2](f)(\sigma) \}$} \\
 \hline
 \thead{$\{S_1\} \square \{S_2\}$} & \thead{$\lambda\sigma. ev[S_1](f)(\sigma) \cup ev[S_2](f)(\sigma)$} \\
 \hline
 \thead{$\texttt{if } (b) \texttt{ then } \{ S_1 \}$ \\ $\texttt{ else } \{ S_2 \}$} & \thead{$\lambda\sigma.\{[\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 \,|\,$ \\$t_1 \in ev[S_1](f)(\sigma), t_2 \in ev[S_2](f)(\sigma) \}$} \\
 \hline
 \thead{$\texttt{while }(b) \texttt{ do }\{S\}$} & \thead{lfp ($\lambda X. (\lambda \sigma. \{[\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 \,|\,$\\$t_1 \in ev[S](X)(\sigma), t_2 \in f(\sigma) \}$))} \\
 \hline
\end{tabular}
\end{center}
\caption{Rules for defining the FRV transformer $ev$}
\label{table:rules_ev}
\end{table}

Consider the running example $P$: $\{ \{x := 2\} \square \{x := 5\} \} [p] \{ x := 7 \}$. The value of $x$ after the execution of $P$ in a given initial state is given by a fuzzy random variable. We want to compute its expected value. Thus, we have to determine $ev[P](\underline{x})$. It is a function of $\sigma$, but the form of $P$ gives the intuition that it will be a constant function, not depending on $\sigma$.\newline
With the rules presented in Table \ref{table:rules_ev}, we can see that this requires first to compute the functions $ev[\{x := 2\} \square \{x := 5\}](\underline{x})$ and $ev[x := 7](\underline{x})$; besides, the former requires to compute $ev[x := 2](\underline{x})$, $ev[x := 5](\underline{x})$.\newline

We get that:
$$ev[x := 2](\underline{x}) = \lambda\sigma.\underline{x}(\sigma[x/2]) = \lambda\sigma.\{x_{\sigma[x/2]}\} = \lambda\sigma.\{2\}$$ 
Indeed, the value of $x$ in $\sigma[x/2]$ is necessarily $2$. Recall that $\sigma[x/2]$ is the state obtained after setting $x$ to $2$ in the state $\sigma$. This means that the expected value of $x$ after executing $x := 2$ in any state $\sigma$ can only be $2$. By the same computation, we get $ev[x := 5](\underline{x}) = \lambda\sigma.\{5\}$ and $ev[x := 7](\underline{x}) = \lambda\sigma.\{7\}$.\newline

Now, we can compute that:
\begin{align*}
ev[\{x := 2\} \square \{x := 5\}](\underline{x}) &= \lambda\sigma. ev[x := 2](\underline{x})(\sigma) \cup ev[x := 5](\underline{x})(\sigma) \\
&= \lambda\sigma. \{2\} \cup \{5\} = \lambda\sigma.\{2,5\} 
\end{align*}
This means that the possible expected values of $x$ after executing $\{x := 2\} \square \{x := 5\}$ in any state $\sigma$ are $2$ and $5$.\newline

Finally, 
\begin{align*}
ev[P_0](\underline{x}) &= \lambda\sigma.\{t_1 p+t_2(1-p) \,|\, t_1 \in ev[\{x := 2\} \square \{x := 5\}](\underline{x})(\sigma), t_2 \in ev[x := 7](\underline{x})(\sigma) \} \\
&= \lambda\sigma.\{t_1 p+t_2(1-p) \,|\, t_1 \in \{2,5\}, t_2 \in \{7\} \} \\
&= \lambda\sigma. \{2p+7(1-p),5p+7(1-p) \} 
\end{align*}

Contrary to what was observed with the semantics of McIver and Morgan, no information is lost here.
The results we just showed means that, when we execute $P_0$ in a state $\sigma$, there are two possible values for the program variable $x$ after the execution. The first one results of the case where $x := 2$ is executed in the nondeterministic choice. In this case, anticipating the value of $x$ is the same as computing the expected value of a classical random variable as we did in Subsection \ref{subsec:anticipation}. The second one is obtained symmetrically in the case where $x := 5$ is executed.\newline
The link with fuzzy random variables is clearer, as the previous results shows that, for each $\sigma$, $ev[P_0](\underline{x})(\sigma)$ is the expected value of a fuzzy random variable taking the value $\{2,5\}$ with a probability $p$ (this corresponds to the first execution scheme, where the left branch of the probabilistic choice is executed. And this case, $x$ has indeed a probability $p$ of being set to $2$ or $5$. Both choices are \underline{possible}, and that is why both values belong to the set), and the value $\{7\}$ with a probability $1-p$.
\bigskip

Finally, we can explain how our semantics is related to possibility theory. Recall that McIver and Morgan extended $wp$ to fully probabilistic programs such that, if $\varphi$ is a predicate, then $wp[P](\varphi)(\sigma)$ is the probability that $P$, when executed in $\sigma$, terminates in a state where $\varphi$ is satisfied.\newline
If we denote by $\mathds{1}_U$ the indicator function of a set $U$, and if we take $f \in \mathcal{F}'$, then $\mathds{1}_{ev[P](f)(\sigma)}(a)$ is the possibility that $a$ is an expected value of $f$ after the execution of $P$ in the state $\sigma$. And we can extend this result: if $V \subset \RRposi$, then $\mathds{1}_{ev[P](f)(\sigma)}(V) = \Sup_{a \in V}(\mathds{1}_{ev[P](f)(\sigma)}(a))$ is the possibility that at least one value in $V$ is an expected value of $f$ after the execution of $P$ in $\sigma$.

\section{Conclusion}
\label{sec:conclusion}
In this report, we presented the vision of weakest-precondition calculus over pGCL program, detailed by McIver and Morgan in \cite{McIver05}. We explained how the weakest-precondition calculus could be used in order to anticipate the values of particular functions (the restriction to positive functions is due to constraints of having a complete partial order over the set of functions in order to ensure the existence of least fixed-points and make the computation possible), and explained the link with random variables, i.e. the value of a function after the execution of program in a given initial state is a random variable, and the weakest precondition calculus gives access to its expected values. We underlined that nevertheless, the results obtained with the semantics of McIver and Morgan were not satisfying with the demonic choice, and neither with the angelic choice, as both conventions omitted essential information. Finally, we presented the notion of fuzzy random variables, and our own semantics, where we anticipate values of functions taking not positive real numbers as values, but set of positive real numbers. We explained the link between our semantics and the expected value of fuzzy random variables.


\bibliography{ref1}{}
\bibliographystyle{plain}

\newpage
\section{Appendix}
\label{sec:appendix}

\subsection{Kleene's Fixed point theorem}
\label{subsec:kleene}

Let $P$ be a set with a partial order $\leq$.
\begin{definition}
$P$ is a \emph{complete partial order} if the following conditions are satisfied:
\begin{itemize}
\item $\leq$ has a minimal element, denoted $\bot$ (such that $\forall x \in P$, $\bot \leq x$)
\item for each subset $S = \{x_1, x_2, \dots \} \subset P$ where $x_1 \leq fx2 \leq \dots $, $S$ has a least upper bound $\Sup(S)$ in $P$ (Such a set $S$ is called a \emph{chain})
\end{itemize}
\end{definition}

\begin{theorem}{\textnormal{Kleene's fixed-point theorem}}
If $P$ is a complete partial order and $f : P \rightarrow P$ is a monotone function (i.e. $\forall x,y \in P$, $x \leq y \Rightarrow f(x) \leq f(y)$), then $f$ has a least fixed-point, and its value is $\textnormal{lfp}(f) = \Sup_{n \in \NN}(f^n(\bot))$
\end{theorem} \todo{Find a reference to the proof}
\todo{Monotone or Scott-Continuous... ? If Scott-Continuous, then I should change all proofs a little, but it should not be a problem)}
\subsection{Weakest precondition calculus in pGCL}
\label{subsec:predicates_cpo}

With the notations of Section \ref{sec:state}, let $\mathcal{P}$ be the set of predicates, and consider the relation $\leq_{\mathcal{P}}$ such that $f \leq_{\mathcal{P}} g$ iff $\forall \sigma \in \Sigma$, $f(\sigma) \leq g(\sigma)$. The order $\leq_{\mathcal{P}}$ is called the \emph{pointwise order}, as the value of $f$ must be inferior to the value of $g$ in each point $\sigma$.\bigskip

We immediately get the following theorem:
\begin{theorem}
$(\mathcal{P},\leq_{\mathcal{P}})$ is a partial order.
\end{theorem}

We will prove that it is even a complete partial order:
\begin{theorem}
$(\mathcal{P},\leq_{\mathcal{P}})$ is a complete partial order
\end{theorem}
\begin{proof}
\begin{itemize}
\item One can easily verify that $\bot$ is the constant function $\sigma \rightarrow 0$
\item Let $S = \{f_1, f_2, \dots \}$ be a subset of $P$ with $f_1 \leq_{\mathcal{P}} f_2 \leq_{\mathcal{P}} \dots$. Then, we define the function $f : \sigma \rightarrow \Sup_{n \in \NN^*}(f_n(\sigma))$. \newline
For each $\sigma$, $f(\sigma)$ is correctly defined: as the set $\{f_n(\sigma) \,|\, n \in \NN^*\}$ is a  subset of $[0,1]$, it has a least upper bound in $[0,1]$. Thus, $f \in \mathcal{P}$
\begin{itemize}
\item Let $n \in \NN^*$. Then, by construction, for each $\sigma \in \Sigma$, $f_n(\sigma) \leq \Sup_{n \in \NN^*}(f_n(\sigma)) = f(\sigma)$. And therefore, $f_n \leq_{\mathcal{P}} f$. As this holds for any $n$, we get that $f$ is an upper bound of $S$
\item Let $g$ be another upper bound of $S$: then, $\forall n \in \NN^*$, $\forall \sigma \in \Sigma$, $f_n(\sigma) \leq g(\sigma)$. Then, $\forall \sigma \in \Sigma$, $f(\sigma) = \Sup_{n \in \NN^*}(f_n(\sigma)) \leq g(\sigma)$. Hence, $f \leq g$. Therefore, $f$ is the least upper bound of $S$
\end{itemize}
\end{itemize}
\end{proof}
 
 Note that, with an analogue proof, if we denote by $\mathcal{F}$ the set of functions $\{f \,|\, f : \Sigma \rightarrow \RRposi\}$ (where $\RRposi = [0;+\infty]$ with $+\infty$ included), and by $\leq_{\mathcal{F}}$ the canonical pointwise order over $\mathcal{F}$, we can prove that $(\mathcal{F},\leq_{\mathcal{F}})$ is a complete partial order. The inclusion of $+\infty$ in the set of values taken by the function is necessary, otherwise the proof does not work, we can find chains for which the $\Sup$ does not exist.
 
\todo{Prove that the function constructed at then end of the Table \ref{table:rules_wp_pgcl} is indeed monotone (or Scott-Continuous ?). To that extent, prove inductively that $wp[P]$ is also monotone (or Scot-contiuous ?)}

\subsection{Application of the semantics McIver and Morgan to $P_0$}
\label{subsec:wp_p0}
Recall that $P_0$ is the program $\{ \{x := 2\} \square \{x := 5\} \} [p] \{ x := 7 \}$. Recall that we denoted by $P_2$ the left member $\{x := 2\} \square \{x := 5\}$. Let $\varphi$ be any postcondition.\bigskip

Looking at the rules in Table \ref{table:rules_wp_pgcl}, one notices that, to compute $wp[P](\varphi)$, one first has to compute $wp[P_2](\varphi)$ and $wp[x := 7](\varphi)$, and to compute the former, we need to compute $wp[x := 2](\varphi)$ and $wp[x := 5](\varphi)$.\bigskip

Applying the assignment rule, we get that:
$$wp[x := 2](\varphi) = \lambda\sigma.\varphi(\sigma[x/2])$$
And symmetrically: 
$$wp[x := 5](\varphi) = \lambda\sigma.\varphi(\sigma[x/5])$$
$$wp[x := 7](\varphi) = \lambda\sigma.\varphi(\sigma[x/7])$$.

Now, applying the nondeterministic choice rule, we get:
\begin{align*}
wp[P_2](\varphi) &= wp[\{x := 2\} \square \{x := 5\}](\varphi) \\
&= \lambda\sigma.\Min(wp[x := 2](\varphi)(\sigma),wp[x := 5](\varphi)(\sigma)) \\
&= \lambda\sigma. \Min(\varphi(\sigma[x/2]),\varphi(\sigma[x/5]))
\end{align*}

And finally, applying the probabilistic choice rule, we get:
\begin{align*}
wp[P_0](\varphi) &= wp[\{ \{x := 2\} \square \{x := 5\} \} [p] \{ x := 7 \}](\varphi)\\
&= \lambda\sigma. p \cdot wp[\{x := 2\} \square \{x := 5\}](\varphi)(\sigma) + (1-p) \cdot wp[x := 7](\varphi)(\sigma)\\
&= \lambda\sigma. p \cdot \Min(\varphi(\sigma[x/2]),\varphi(\sigma[x/5])) + (1-p) \cdot \varphi(\sigma[x/7])
\end{align*}

Now, we can try replacing $\varphi$ by interesting postconditions.
\begin{itemize}
\item If $\varphi = [x = 7]$, then for each $\sigma$, we get $[x = 7](\sigma[x/2]) = 0$, $[x = 7](\sigma[x/5]) = 0$ and $[x = 7](\sigma[x/7]) = 1$, and therefore, $wp[P_0]([x = 7]) = \lambda\sigma. (1-p)$
\item If $\varphi = [x = 2]$, then for each $\sigma$, we get $[x = 2](\sigma[x/2]) = 1$, $[x = 2](\sigma[x/5]) = 0$ and $[x = 2](\sigma[x/7]) = 0$, and therefore, $wp[P_0]([x = 2]) = \lambda\sigma. 0$ (and symmetrically, $wp[P_0]([x = 5]) = \lambda\sigma. 0$)
\item If $\varphi = [x = 2 \textnormal{ OR } x = 5]$, we get $[x = 2 \textnormal{ OR } x = 5](\sigma[x/2]) = 1$, $[x = 2 \textnormal{ OR } x = 5](\sigma[x/5]) = 1$ and $[x = 2 \textnormal{ OR } x = 5](\sigma[x/7]) = 0$, and therefore, $wp[P_0]([x = 2 \textnormal{ OR } x = 5]) = \lambda\sigma. p$
\end{itemize}
These are precisely the results stated in Section \ref{sec:intro}.

\subsection{Example of anticipation of values in Section \ref{subsec:anticipation}}
\label{subsec:ex_ant}

\begin{example}
\label{ex:diff_rv}
Here is an example of program where the random variables associated to the anticipated value differ, depending on the initial state.
$$P_3 : \{x := y+1\} [p] \{x := y-1\}$$
The value of $y$ is unknown and depends on the initial state.
Let $\sigma \in \Sigma$ be an initial state. Consider that we want to anticipate the value of the program variable $x$ (and therefore, of the function $\underline{x}$). 
Two execution schemes are possible:
\begin{itemize}
\item $\omega_1$, with probability $p$, executes $x := y+1$
\item $\omega_2$, with probability $1-p$, executes $x := y-1$
\end{itemize}
When computing the anticipated value of $\underline{x}$, we get:
\begin{align*}
wp[P_3](\underline{x})(\sigma) &= p\cdot wp[x := y+1](\underline{x})(\sigma)+(1-p)\cdot wp[x := y-1](\underline{x})(\sigma)\\
&= p \cdot \underline{x}(\sigma[x/y+1])+(1-p)\cdot \underline{x}(\sigma[x/y-1]) \\
&= p(y_{\sigma}+1)+(1-p)(y_{\sigma}-1)
\end{align*}
Therefore:
$$wp[P_3](\underline{x}) = p(\underline{y}+1)+(1-p)(\underline{y}-1)$$
In this example, it is clear that for each $\sigma$, $wp[P_3](\underline{x})$ appears as the expected value of a random variable, taking the value $(y_{\sigma}+1)$ with probability $p$ and the value $(y_{\sigma}-1)$ with probability $1-p$. However, this random variable depends on $\sigma$. Therefore, when you change $\sigma$, you change the random variable.
\end{example}

\begin{example}
\label{ex:diff_ex_sch}
Here is an example of program where the execution scheme differ, depending on the postcondition.
Consider the program $P_0$ defined in Section $1$.
\begin{itemize}
\item When considering as postcondition the predicate $[x = 2]$, we get two execution schemes:
\begin{itemize}
\item $\omega_1$ (executed with a probability $p$), executing the left part of the probabilistic choice, and then automatically picking the least desired outcome in the nondeterministic choice, i.e. $x := 5$
\item $\omega_2$ (executed with a probability $1-p$), executing the right part of the probabilistic choice
\end{itemize}
\item When considering as postcondition the predicate $[x = 5]$, we get two other execution schemes:
\begin{itemize}
\item $\omega_1$ (executed with a probability $p$), executing the left part of the probabilistic choice, and then automatically picking the least desired outcome in the nondeterministic choice, i.e. $x := 2$
\item $\omega_2$ (executed with a probability $1-p$), executing the right part of the probabilistic choice
\end{itemize}
\end{itemize}
\end{example}

\subsection{Anticipation of values of set-valued functions in Section \ref{sec:contribution}}
Recall that we defined $\mathcal{F}' = \{f : \,|\, f : \Sigma \rightarrow \mathcal{P}(\RRposi) \}$, and we defined the semantics of the pGCL program $P$ as a transformer $ev[P]$, transforming $f \in \mathcal{F'}$ in $ev[P](f) \in \mathcal{F}'$. But for the definition of $ev$ to make sense (i.e. for the least fixed-point in the loop rule to exist, by applying Kleene's Theorem), we want to prove two points:
\begin{enumerate}
\item $\mathcal{F}'$ is a complete partial order.
\item For all programs $P$, the function transformer $ev[P] : \mathcal{F}' \rightarrow \mathcal{F}'$ is monotone (this is sufficient to make the function whose least fixed-point is calculated monotone)
\end{enumerate}
\bigskip

We define the lower closure of a subset of $\RRposi$.
\begin{definition}
If $U$ is a subset of $\RRposi$, then the lower closure $\downarrow U$ of $U$ is defined as:
$$x \in \downarrow U \textnormal{ iff } \exists y \in U, x \leq y$$
\end{definition}
We immediately get the following lemma, which will be used a lot in our proofs:
\begin{lemma}
\label{magical_lemma}

\begin{enumerate}
\item If $U$ is a subset of $\RRposi$, then $U \subset \downarrow U$.
\item $\downarrow U = \emptyset$ iff $U = \emptyset$ 
\end{enumerate}
\end{lemma}

\begin{theorem} The following order is a partial order over $\mathcal{F}'$:
$$f \leq_{\mathcal{F}'} g \textnormal{ iff } \forall \sigma \in \Sigma, \downarrow(f(\sigma)) \subset \downarrow(g(\sigma))$$
\end{theorem}
\begin{proof} Reflexivity, antisymmetry and transitivity are respectively obtained from the reflexivity, antisymmetry and transitivity of $\subset$ as an order relation over $\RRposi$.
\end{proof}

\begin{theorem}
$(\mathcal{F}',\leq_{\mathcal{F}'})$ is a complete partial order.
\end{theorem}
\begin{proof}
\begin{itemize}
\item One easily sees that $\bot : \sigma \rightarrow \emptyset$ is the minimal element
\item Let $S = \{f_1, f_2, \dots \}$ be a chain of $\mathcal{F}'$. Therefore, we have $f_1 \leq_{\mathcal{F}'} f_2 \leq_{\mathcal{F}'} \dots$.\newline
We shall consider $f \in \mathcal{F}'$ defined by: $f : \sigma \rightarrow \cup_{n \in \NN^*} f_n(\sigma)$. We will prove that $f$ is the least upper bound of $S$.
\begin{itemize}
\item Let $n \in \NN^*$ and $\sigma \in \Sigma$. We have to prove that $\downarrow(f_n(\sigma)) \subset \downarrow (f(\sigma))$. If $f_n(\sigma) = \emptyset$, then we immediately get the result.\newline 
Otherwise, let $x \in \downarrow(f_n(\sigma))$. Therefore, there exists $y \in f_n(\sigma)$ such that $x \leq y$. We immediately deduce that $y \in f_n(\sigma) \in \cup_{n \in \NN^*} f_n(\sigma) = f(\sigma)$, and as $x \leq y$, then $x \in \downarrow (f(\sigma))$. As this holds for all $n \in \NN^*$ and for all $\sigma \in \Sigma$, we get that:
$$\forall n \in \NN^*, \forall \sigma \in \Sigma, \downarrow(f_n(\sigma)) \subset \downarrow(f(\sigma))$$
And therefore, $\forall n \in \NN^*$, $f_n \leq_{\mathcal{F}'} f$. Thus, $f$ is an upper bound for $S$.
\item Let $g$ be another upper bound of $S$. Therefore, we have:
$$\forall n \in \NN^*, \forall \sigma \in \Sigma, \downarrow(f_n(\sigma)) \subset \downarrow(g(\sigma))$$
Let $\sigma \in \Sigma$. We want to prove that $\downarrow (f(\sigma)) \subset \downarrow(g(\sigma))$. If $f(\sigma) = \emptyset$, the result is immediate.\newline
Otherwise, let $x \in \downarrow(f(\sigma))$. Therefore, there exists $y \in f(\sigma)$ such that $x \leq y$. As $y \in f(\sigma)$, there exists $n \in \NN^*$ such that $y \in f_n(\sigma)$. With the Lemma \ref{magical_lemma}, we get that $y \in \downarrow(f_n(\sigma))$, and then, with the previous hypothesis, $y \in \downarrow(g(\sigma))$. Hence, there exists $z \in g(\sigma)$ with $y \leq z$. Thus, $x \leq z$, and therefore, $x \in \downarrow(g(\sigma))$.\newline
Therefore, $\forall \sigma \in \Sigma$, $\downarrow(f(\sigma)) \subset \downarrow(g(\sigma))$, and thus, $f \leq_{\mathcal{F}'} g$. Hence, $f$ is the least upper bound of $S$.
\end{itemize} 
\end{itemize}
\end{proof}

We proved the first point: $\mathcal{F}'$ is a complete partial order.\newline
Now, we want to prove the following theorem.

\begin{theorem}
For all pGCL programs $P$, $ev[P] : \mathcal{F}' \rightarrow \mathcal{F}'$ is a monotone function
\end{theorem}
\begin{proof}
We will prove it by induction, using the rules in Table \ref{table:rules_ev}.
\begin{itemize}
\item Obviously, when $P = \texttt{skip}$, if $f \leq_{\mathcal{F}'} g$, then $ev[skip](f) = f \leq_{\mathcal{F}'} g = ev[skip](g)$\bigskip

\item $P = \{ x := A\}$, if $f \leq_{\mathcal{F}'} g$, then we get that $\forall \sigma \in \Sigma$, $\downarrow(f(\sigma)) \subset \downarrow (g(\sigma))$. Particularly, for all $\sigma' \in \Sigma$ such that there exists $\sigma \in \Sigma$ where $\sigma' = \sigma[x/A]$, we get $\downarrow(f(\sigma')) \subset \downarrow(g(\sigma'))$. Therefore, $ev[x := A](f) = \lambda \sigma. f(\sigma[x/A]) \leq_{\mathcal{F}'} \lambda \sigma. g(\sigma[x/A]) = ev[x := A](g)$
\bigskip

\item When $P = P_1;P_2$, assume that $ev[P_1]$ and $ev[P_2]$ are monotone. Then, if $f \leq_{\mathcal{F}'} g$, we get that $ev[P_2](f) \leq_{\mathcal{F}'} ev[P_2](g)$, and then, $ev[P_1](ev[P_2](f)) \leq_{\mathcal{F}'} ev[P_1](ev[P_2](g))$. Finally, we proved that $ev[P_1 ; P_2](f) \leq_{\mathcal{F}'} ev[P_1 ; P_2](g)$
\bigskip

\item When $P = P_1 \square P_2$, assume that $ev[P_1]$ and $ev[P_2]$ are monotone. Then, if $f \leq_{\mathcal{F}'} g$, we shall prove that $ev[P](f) = \lambda\sigma. ev[P_1](f)(\sigma) \cup ev[P_2](f)(\sigma) \leq_{\mathcal{F}'} \lambda\sigma. ev[P_1](g)(\sigma) \cup ev[P_2](g)(\sigma) = ev[P](g)$. \newline
To that extent, we have to prove that, 
$$\forall \sigma \in \Sigma, \downarrow(ev[P_1](f)(\sigma) \cup ev[P_2](f)(\sigma)) \subset \downarrow(ev[P_1](g)(\sigma) \cup ev[P_2](g)(\sigma))$$
Let $\sigma \in \Sigma$. If $ev[P_1](f)(\sigma) \cup ev[P_2](f)(\sigma) = \emptyset$, then there is nothing to prove.\bigskip

Otherwise, let $x \in \downarrow(ev[P_1](f)(\sigma) \cup ev[P_2](f)(\sigma))$. Then, there exists $y \in ev[P_1](f)(\sigma) \cup ev[P_2](f)(\sigma)$ such that $x \leq y$. We assume that $y \in ev[P_1](f)(\sigma)$. Then, with the Lemma \ref{magical_lemma}, $y \in \downarrow (ev[P_1](f)(\sigma)) \subset \downarrow (ev[P_1](g)(\sigma))$. Therefore, there exists $z \in ev[P_1](g)(\sigma)$ such that $y \leq z$. One the one hand, we can notice that $z \in ev[P_1](g)(\sigma) \subset ev[P_1](g)(\sigma) \cup ev[P_2](g)(\sigma)$, and on the other hand, that $x \leq y \leq z$. Therefore, $x \in \downarrow(ev[P_1](g)(\sigma) \cup ev[P_2](g)(\sigma))$. The proof is symmetrical if $y \in ev[P_2](f)(\sigma)$. Therefore, we got our result.\bigskip

\item When $P = P_1 [p] P_2$, assume that $ev[P_1]$ and $ev[P_2]$ are monotone. Then, if $f \leq_{\mathcal{F}'} g$, we shall prove that $ev[P](f) = \lambda \sigma. \{t_1p+t_2(1-p) \,|\, t_1 \in ev[P_1](f)(\sigma), t_2 \in ev[P_2](f)(\sigma)\} \leq_{\mathcal{F}'} \lambda \sigma. \{t_1p+t_2(1-p) \,|\, t_1 \in ev[P_1](g)(\sigma), t_2 \in ev[P_2](g)(\sigma)\} = ev[P](g)$ .\newline
To that extent, we have to prove that:
\begin{align*}
\forall \sigma \in \Sigma, & \downarrow (\{t_1p+t_2(1-p) \,|\, t_1 \in ev[P_1](f)(\sigma), t_2 \in ev[P_2](f)(\sigma)\}) \\
\subset & \downarrow (\{t_1p+t_2(1-p) \,|\, t_1 \in ev[P_1](g)(\sigma), t_2 \in ev[P_2](g)(\sigma)\})
\end{align*}
Let $\sigma \in \Sigma$. As usual, if  $\{t_1p+t_2(1-p) \,|\, t_1 \in ev[P_1](f)(\sigma), t_2 \in ev[P_2](f)(\sigma)\} = \emptyset$, there is nothing to prove.\bigskip

Otherwise, let $x \in \downarrow (\{t_1p+t_2(1-p) \,|\, t_1 \in ev[P_1](f)(\sigma), t_2 \in ev[P_2](f)(\sigma)\})$. Then, there exists $y \in \{t_1p+t_2(1-p) \,|\, t_1 \in ev[P_1](f)(\sigma), t_2 \in ev[P_2](f)(\sigma)\}$ such that $x \leq y$. We can tell that there exists $t_1 \in ev[P_1](f)(\sigma)$ and $t_2 \in ev[P_1](2)(\sigma)$ such that $y = t_1p+t_2(1-p)$.\newline
With the Lemma \ref{magical_lemma}, $t_1 \in ev[P_1](f)(\sigma) \subset \downarrow ( ev[P_1](f)(\sigma)) \subset \downarrow (ev[P_1](g)(\sigma))$, and therefore, there exists $z_1 \in ev[P_1](g)(\sigma)$ such that $t_1 \leq z_1$. Symmetrically, there exists $z_2 \in ev[P_2](g)(\sigma)$ such that $t_2 \leq z_2$.\bigskip

Now, we have $x \leq y = t_1p+t_2(1-p) \leq z_1p+z_2(1-p)$, with $z_1 \in ev[P_2](g)(\sigma)$ and $z_2 \in ev[P_2](g)(\sigma)$.\newline
This proves that $x \in \downarrow (\{t_1p+t_2(1-p) \,|\, t_1 \in ev[P_1](g)(\sigma), t_2 \in ev[P_2](g)(\sigma)\})$.\bigskip

\item When $P = \texttt{ if } b \texttt{ then } P_1 \texttt{ else } P_2$, assume that $ev[P_1]$ and $ev[P_2]$ are monotone. Then, if $f \leq_{\mathcal{F}'} g$, we shall prove that $ev[P](f) = \lambda\sigma.\{[\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 \,|\,t_1 \in ev[P_1](f)(\sigma), t_2 \in ev[P_2](f)(\sigma) \} \leq_{\mathcal{F}'} \lambda\sigma.\{[\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 \,|\, t_1 \in ev[P_1](g)(\sigma), t_2 \in ev[P_2](g)(\sigma) \} = ev[P](g)$. \newline
To that extent, we have to prove that:
\begin{align*}
\forall \sigma \in \Sigma, & \downarrow \{[\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma) \cdot t_2 \,|\, \\
& t_1 \in ev[P_1](f)(\sigma), t_2 \in ev[P_2](f)(\sigma) \} \\
\subset & \downarrow  \{ [\![b : true ]\!](\sigma) \cdot t_1 + [\![b : false ]\!](\sigma) \cdot t_2 \,|\, \\
& t_1 \in ev[P_1](g)(\sigma), t_2 \in ev[P_2](g)(\sigma) \} \\
\end{align*}
Let $\sigma \in \Sigma$. If $\{[\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 \,|\, t_1 \in ev[P_1](f)(\sigma), t_2 \in ev[P_2](f)(\sigma) \} = \emptyset$, then the result is immediate.\bigskip

Otherwise, let $x \in \downarrow \{[\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 \,|\, t_1 \in ev[P_1](f)(\sigma), t_2 \in ev[P_2](f)(\sigma) \} $.\newline
Then, there exists $t_1 \in ev[P_1](f)(\sigma)$ and $t_2 \in ev[P_2](f)(\sigma)$ such that $x \leq  [\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 $. Let $y = [\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2
$\bigskip

Using the Lemma \ref{magical_lemma}, we get that $t_1 \in ev[P_1](f)(\sigma) \subset \downarrow(ev[P_1](f)(\sigma)) \subset \downarrow(ev[P_1](g)(\sigma))$. Therefore, there exists $z_1 \in ev[P_1](g)(\sigma)$ such that $t_1 \leq z_1$. And symmetrically, there exists $z_2 \in ev[P_2](g)(\sigma)$ such that $t_2 \leq z_2$.\newline

Let $z = [\![b : true ]\!](\sigma) \cdot z_1 + [\![b : false ]\!](\sigma) \cdot z_2$.\newline
If $b$ holds in $\sigma$, then $y = t_1$ and $z = z_1$. We had $x \leq y$, and $t_1 \leq z_1$. Thus, $x \leq z$. Symmetrically, if $b$ does not hold in $\sigma$, then, $y = t_2$ and $z = z_2$ and we symmetrically prove that $x \leq z$. And we easily see that $z \in \{ [\![b : true ]\!](\sigma) \cdot t_1 + [\![b : false ]\!](\sigma) \cdot t_2 \,|\,
t_1 \in ev[P_1](g)(\sigma), t_2 \in ev[P_2](g)(\sigma) \}$. This concludes this point of the proof.\bigskip

\item When $P = \texttt{ while }(b)\texttt{ do }\{P_1\}$, assume that $ev[P_1]$ is monotone. Then, if $f \leq_{\mathcal{F}'} g$, we shall prove that $ev[P](f) =$ lfp $(\lambda X. (\lambda \sigma. \{[\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 \,|\, t_1 \in ev[P_1](X)(\sigma), t_2 \in f(\sigma) \}) \leq_{\mathcal{F}'}$ lfp $(\lambda X. (\lambda \sigma. \{[\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 \,|\, t_1 \in ev[P_1](X)(\sigma), t_2 \in g(\sigma) \}) =ev[P](g)$. \bigskip

We proved previously that $\mathcal{F}'$ was a complete partial order. Therefore, Kleene's fixed-point theorem can be applied here, and we can compute the two least fixed-point previously mentioned.\bigskip

Let $F_f(X)$ denote $\lambda X. (\lambda \sigma. \{[\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 \,|\, t_1 \in ev[P_1](X)(\sigma), t_2 \in f(\sigma) \}$, and symmetrically, let $F_g(X)$ denote $\lambda X. (\lambda \sigma. \{[\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 \,|\, t_1 \in ev[P_1](X)(\sigma), t_2 \in g(\sigma) \}$.\bigskip

We seek to prove that lfp $F_f \leq_{\mathcal{F}'} $ lfp $ F_g$. Applying Kleene's Theorem (recall that $\bot : \sigma \rightarrow \emptyset$), we get that 
\begin{align*}
\textnormal{lfp } F_f = \Sup_{n \in \NN^*}(F_f^n(\bot)) \\
\textnormal{lfp } F_g = \Sup_{n \in \NN^*}(F_g^n(\bot))
\end{align*}
Therefore, we have to prove that:
\begin{align*}
\forall \sigma \in \Sigma, \downarrow ((\Sup_{n \in \NN^*}(F_f^n(\bot)))(\sigma)) \subset  \downarrow ((\Sup_{n \in \NN^*}(F_g^n(\bot)))(\sigma)) 
\end{align*}
One easily verifies that, to prove the previous proposition, it is sufficient to prove that:
\begin{align*}
\forall n \in \NN, F_f^n(\bot) \leq_{\mathcal{F}'} F_g^n(\bot)
\end{align*}
Or, equivalently:
\begin{align*}
\forall n \in \NN, \forall \sigma \in \Sigma, \downarrow(F_f^n(\bot)(\sigma)) \subset \downarrow(F_g^n(\bot)(\sigma))
\end{align*}
This is proven by induction:
\begin{itemize}
\item For $n = 0$, $F_f^0 = F_g^0 = Id_{\mathcal{F}'}$, and therefore, the result is immediate.
\item If, for one $n \in \NN$, we get that $\forall \sigma \in \Sigma, \downarrow(F_f^n(\bot)(\sigma)) \subset \downarrow(F_g^n(\bot)(\sigma))$, then, let $\sigma \in \Sigma$ and let $x \in \downarrow(F_f^{n+1}(\bot)(\sigma))$. We shall prove that $x \in \downarrow(F_g^{n+1}(\bot)(\sigma))$.\bigskip

We know that there exists $y \in F_f^{n+1}(\bot)(\sigma)$ such that $x \leq y$. And as $y \in F_f^{n+1}(\bot)(\sigma) = F_f(F_f^n(\bot))(\sigma) = \{[\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 \,|\, t_1 \in ev[P_1](F_f^n(\bot))(\sigma), t_2 \in f(\sigma) \}$. \bigskip

Hence, there exists $t_1 \in ev[P_1](F_f^n(\bot))(\sigma)$ and $t_2 \in f(\sigma)$ such that $y = [\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 $.\newline
With the Lemma \ref{magical_lemma} we get that $t_1 \in ev[P_1](F_f^n(\bot))(\sigma) \subset \downarrow(ev[P_1](F_f^n(\bot))(\sigma))$.\newline
With the induction hypothesis, that $F_f^n(\bot) \leq_{\mathcal{F}'} F_g^n(\bot)$, and with the monotonicity of $ev[P_1]$, we get that $ev[P_1](F_f^n(\bot)) \leq_{\mathcal{F}'} ev[P_1](F_g^n(\bot))$, and particularly, that $\downarrow(ev[P_1](F_f^n(\bot))(\sigma)) \subset \downarrow(ev[P_1](F_g^n(\bot))(\sigma))$. Hence, $t_1 \in \downarrow(ev[P_1](F_g^n(\bot))(\sigma))$.\newline
Symetrically, recall that $f \leq_{\mathcal{F}'} g$, and therefore, $\downarrow(f(\sigma)) \subset \downarrow(g(\sigma))$. And as $t_2 \in f(\sigma)$, then $t_2 \in \downarrow(f(\sigma))$, and hence, $t_2 \in \downarrow(g(\sigma))$.\bigskip

Hence, e get that there exists $z_1 \in ev[P_1](F_g^n(\bot))(\sigma)$ and $z_2 \in g(\sigma)$ such that $t_1 \leq z_1$ and $t_2 \leq z_2$.\newline
Therefore, $y = [\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 \leq [\![b : true ]\!](\sigma)\cdot z_1 + [\![b : false ]\!](\sigma)\cdot z_2 \in F_g(F_g^n(\bot))(\sigma) = F_g^{n+1}(\sigma)$. Therefore, $y \in \downarrow(F_g^{n+1}(\sigma))$. And as $x \leq y$, then $x \in \downarrow(F_g^{n+1}(\sigma))$.\bigskip

This concludes the proof.
\end{itemize}
\end{itemize} 

%lfp ($\lambda X. (\lambda \sigma. \{[\![b : true ]\!](\sigma)\cdot t_1 + [\![b : false ]\!](\sigma)\cdot t_2 \,|\,$\\$t_1 \in ev[S](X)(\sigma), t_2 \in f(\sigma) \}$))

\end{proof}

\end{document}
